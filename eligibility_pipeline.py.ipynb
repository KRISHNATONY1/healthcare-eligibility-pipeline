{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc320d6-0380-4b16-ab3e-19ead39b1578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4035dd-002c-435f-9d9d-1980e19de832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!ls Data/acme\n",
    "!ls Data/bettercare\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7d08b9c4-b4b4-4e71-a7fd-0986d58be893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e389f6c3-dc1b-4ef4-91ef-9c2c837d454e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### This script builds a unified healthcare eligibility dataset by ingesting and standardizing data from multiple partner sources with different file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0add8d2-69c6-4a4a-b27f-3f13f01c33cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Healthcare Eligibility Pipeline (FINAL)\n",
    "# =========================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, initcap, lower,\n",
    "    regexp_replace, concat_ws,\n",
    "    lit, coalesce, try_to_date\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EligibilityPipeline\").getOrCreate()\n",
    "\n",
    "# -------------------------\n",
    "# Partner Configuration\n",
    "# -------------------------\n",
    "\n",
    "PARTNER_CONFIG = {\n",
    "    \"acme\": {\n",
    "        \"file_path\": \"/Workspace/Users/vijaylanka18us@gmail.com/Data Engineer Assessment/Data/acme/acme.txt\",\n",
    "        \"file_type\": \"csv\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"partner_code\": \"ACME\",\n",
    "        \"column_mapping\": {\n",
    "            \"MBI\": \"external_id\",\n",
    "            \"FNAME\": \"first_name\",\n",
    "            \"LNAME\": \"last_name\",\n",
    "            \"DOB\": \"dob\",\n",
    "            \"EMAIL\": \"email\",\n",
    "            \"PHONE\": \"phone\"\n",
    "        }\n",
    "    },\n",
    "    \"bettercare\": {\n",
    "        \"file_path\": \"/Workspace/Users/vijaylanka18us@gmail.com/Data Engineer Assessment/Data/bettercare/bettercare.xlsx\",\n",
    "        \"file_type\": \"excel\",\n",
    "        \"partner_code\": \"BETTERCARE\",\n",
    "        \"column_mapping\": {\n",
    "            \"subscriber_id\": \"external_id\",\n",
    "            \"first_name\": \"first_name\",\n",
    "            \"last_name\": \"last_name\",\n",
    "            \"date_of_birth\": \"dob\",\n",
    "            \"email\": \"email\",\n",
    "            \"phone\": \"phone\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1dcea5f-fafb-4db1-ab0b-8e2710cf7f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### This section safely reads input files while handling format-specific quirks and avoiding Arrow conversion issues commonly seen with mixed or messy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12952f3-0aba-47ff-9685-ed24a73db5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Safe Reader (Arrow-safe)\n",
    "# -------------------------\n",
    "def read_partner_file(file_path, file_type, delimiter=None):\n",
    "    if file_type == \"csv\":\n",
    "        pdf = pd.read_csv(file_path, delimiter=delimiter)\n",
    "    elif file_type == \"excel\":\n",
    "        pdf = pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    ### Critical: normalize types for Arrow\n",
    "    pdf = pdf.astype(str).replace(\"nan\", None)\n",
    "\n",
    "    return spark.createDataFrame(pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b9b879-7737-4373-b18e-b67d02acb7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Applies consistent data cleaning and normalization logic so all partner records conform to a single, well-defined eligibility schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9b2835-cc55-4acf-aa90-11ef85d9831b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Standard Transform\n",
    "# -------------------------\n",
    "def transform_to_standard(df, config):\n",
    "\n",
    "    for src, tgt in config[\"column_mapping\"].items():\n",
    "        df = df.withColumnRenamed(src, tgt)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"first_name\", initcap(col(\"first_name\")))\n",
    "        .withColumn(\"last_name\", initcap(col(\"last_name\")))\n",
    "        .withColumn(\"email\", lower(col(\"email\")))\n",
    "        .withColumn(\"email\", regexp_replace(col(\"email\"), \",$\", \"\"))\n",
    "\n",
    "        #  date parsing\n",
    "        .withColumn(\n",
    "            \"dob\",\n",
    "            coalesce(\n",
    "                try_to_date(col(\"dob\"), \"MM/dd/yyyy\"),\n",
    "                try_to_date(col(\"dob\"), \"yyyy-MM-dd\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Clean phone\n",
    "        .withColumn(\"phone\", regexp_replace(col(\"phone\"), \"[^0-9]\", \"\"))\n",
    "        .withColumn(\n",
    "            \"phone\",\n",
    "            concat_ws(\n",
    "                \"-\",\n",
    "                col(\"phone\").substr(1, 3),\n",
    "                col(\"phone\").substr(4, 3),\n",
    "                col(\"phone\").substr(7, 4)\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"partner_code\", lit(config[\"partner_code\"]))\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        df\n",
    "        .select(\n",
    "            \"external_id\",\n",
    "            \"first_name\",\n",
    "            \"last_name\",\n",
    "            \"dob\",\n",
    "            \"email\",\n",
    "            \"phone\",\n",
    "            \"partner_code\"\n",
    "        )\n",
    "        .filter(col(\"external_id\").isNotNull())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e38aafaa-063b-4109-a880-1d3630c4f916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Orchestrates the end-to-end flow by iterating through partners, invoking the appropriate reader, applying transformations, and combining results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f24f1d47-0cb6-4568-91d1-22e0d6474b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Pipeline Runner\n",
    "# -------------------------\n",
    "\n",
    "def run_pipeline():\n",
    "    dfs = []\n",
    "\n",
    "    for partner, config in PARTNER_CONFIG.items():\n",
    "        print(f\"Processing partner: {partner}\")\n",
    "        print(f\"Reading file: {config['file_path']}\")\n",
    "\n",
    "        raw_df = read_partner_file(\n",
    "            config[\"file_path\"],\n",
    "            config[\"file_type\"],\n",
    "            config.get(\"delimiter\")\n",
    "        )\n",
    "\n",
    "        std_df = transform_to_standard(raw_df, config)\n",
    "        dfs.append(std_df)\n",
    "\n",
    "    final_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        final_df = final_df.unionByName(df)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3608305-8a8a-4518-b652-e8f5015f508a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Executes the full pipeline and produces the final, validated eligibility dataset ready for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03797b9-1e94-4204-9ec1-26b601413d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Execute\n",
    "# -------------------------\n",
    "\n",
    "final_df = run_pipeline()\n",
    "final_df.show(truncate=False)\n",
    "final_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6555303910152726,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "eligibility_pipeline.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
